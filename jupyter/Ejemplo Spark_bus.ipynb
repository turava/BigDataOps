{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Contar c\u00faantos trayectos se han realizado en bus a cada uno de los destinos. Spark\n\nEn este ejemplo, no se va a tener en cuenta los campos vacios, lo cual se deber\u00eda de tratar pero para simplificar vamos simplemente a saber la cantidad de trayectos a dicho destino \"sin importar su origen\"."}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# Importamos SparkSession, el punto de entrada clave para funcionalidades de Spark.\n# A diferencia del enfoque con RDD, aqu\u00ed no requerimos el SparkContext expl\u00edcitamente.\n# obtener el SparkContext:\nfrom pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[Row(trip_code='554e2d76', company='7980c0ae', line_number='83ed87ea', bus='3b3307d2', trip_type='regular', origin='TRES RIOS', origin_state='RJ', destination='JUIZ DE FORA', destination_state='MG', year='2019', month='1', day='1', trip_start_hour_and_minute='528', hour='5', minute='28', trip_end_hour_and_minute='652', trip_duration_hours='1.39', travelled_distance_km='67.97099999999999', delay_start_minutes=None, delay_end_minutes='-47.55'), Row(trip_code='c5f130e7', company='7980c0ae', line_number='83ed87ea', bus='651283f4', trip_type='regular', origin='TRES RIOS', origin_state='RJ', destination='JUIZ DE FORA', destination_state='MG', year='2019', month='1', day='1', trip_start_hour_and_minute='530', hour='5', minute='30', trip_end_hour_and_minute='653', trip_duration_hours='1.39', travelled_distance_km=None, delay_start_minutes='0.0166666666666666', delay_end_minutes='-45.983333333')]\n+---------+--------+-----------+--------+---------+---------+------------+------------+-----------------+----+-----+---+--------------------------+----+------+------------------------+-------------------+---------------------+-------------------+-----------------+\n|trip_code| company|line_number|     bus|trip_type|   origin|origin_state| destination|destination_state|year|month|day|trip_start_hour_and_minute|hour|minute|trip_end_hour_and_minute|trip_duration_hours|travelled_distance_km|delay_start_minutes|delay_end_minutes|\n+---------+--------+-----------+--------+---------+---------+------------+------------+-----------------+----+-----+---+--------------------------+----+------+------------------------+-------------------+---------------------+-------------------+-----------------+\n| 554e2d76|7980c0ae|   83ed87ea|3b3307d2|  regular|TRES RIOS|          RJ|JUIZ DE FORA|               MG|2019|    1|  1|                       528|   5|    28|                     652|               1.39|    67.97099999999999|               null|           -47.55|\n| c5f130e7|7980c0ae|   83ed87ea|651283f4|  regular|TRES RIOS|          RJ|JUIZ DE FORA|               MG|2019|    1|  1|                       530|   5|    30|                     653|               1.39|                 null| 0.0166666666666666|    -45.983333333|\n+---------+--------+-----------+--------+---------+---------+------------+------------+-----------------+----+-----+---+--------------------------+----+------+------------------------+-------------------+---------------------+-------------------+-----------------+\nonly showing top 2 rows\n\n"}], "source": "import pyspark.sql.functions as F\nspark = SparkSession.builder.appName(\"ejemplo_DF\")\\\n .getOrCreate() # Si ya existe una SparkSession, getOrCreate() la recupera, en lugar de crear una nueva.\n\n# Aqu\u00ed procedemos a la carga de los datos desde un archivo en un DataFrame.\n# Esto se realiza utilizando las capacionalidades de lectura de Spark.\n# Utilizamos la opci\u00f3n 'header' para evitar cargar la primera fila como datos, una ventaja sobre el m\u00e9todo de manejo de RDD.\nbus_trips_lines = spark.read\\\n .option(\"header\", \"true\")\\\n .csv(\"/Enric/bus_trips.csv\")\n\n# Para visualizar los datos en DataFrames, disponemos de dos m\u00e9todos:\n# El m\u00e9todo 'take' heredado de RDD, as\u00ed como el m\u00e9todo 'show' propio de DataFrames.\n# Ambos son \u00fatiles para inspeccionar r\u00e1pidamente los primeros registros.\nprint(bus_trips_lines.take(2)) # Uso de 'take' para obtener los primeros 2 registros.\nbus_trips_lines.show(2) # Uso de 'show' para visualizar los primeros 2 registros."}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- trip_code: string (nullable = true)\n |-- company: string (nullable = true)\n |-- line_number: string (nullable = true)\n |-- bus: string (nullable = true)\n |-- trip_type: string (nullable = true)\n |-- origin: string (nullable = true)\n |-- origin_state: string (nullable = true)\n |-- destination: string (nullable = true)\n |-- destination_state: string (nullable = true)\n |-- year: string (nullable = true)\n |-- month: string (nullable = true)\n |-- day: string (nullable = true)\n |-- trip_start_hour_and_minute: string (nullable = true)\n |-- hour: string (nullable = true)\n |-- minute: string (nullable = true)\n |-- trip_end_hour_and_minute: string (nullable = true)\n |-- trip_duration_hours: string (nullable = true)\n |-- travelled_distance_km: string (nullable = true)\n |-- delay_start_minutes: string (nullable = true)\n |-- delay_end_minutes: string (nullable = true)\n\n"}], "source": "# Un recurso valioso de la API estructurada de Spark es 'printSchema'.\n# Este m\u00e9todo nos permite examinar r\u00e1pidamente los tipos de datos de las columnas,\n# facilitando la comprensi\u00f3n de la estructura del DataFrame.\nbus_trips_lines.printSchema()"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "# Estas imports ser\u00e1n utilizados para crear una funci\u00f3n personalizada que se integren con el DataFrame.\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\n# Definimos una funci\u00f3n para convertir de manera segura cadenas a enteros.\n# En caso de error en la conversi\u00f3n, retornar\u00e1 None en vez de lanzar una excepci\u00f3n.\ndef safe_int_conversion(s):\n    try:\n        return int(s)\n    except ValueError:\n        return None\n\n# Registramos la funci\u00f3n definida como una UDF en Spark.\n# Esto nos permite aplicarla en una columna de DataFrame.\nsafe_int_conversion_udf = udf(safe_int_conversion, IntegerType())"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "# Transformamos el DataFrame aplicando la UDF a las columnas 'year', 'month' y 'day'.\n# Esto asegura que los datos en estas columnas sean enteros, manejando adecuadamente los valores no num\u00e9ricos.\n# Adem\u00e1s, seleccionamos las columnas relevantes para nuestro an\u00e1lisis.\nbus_trips_lines = bus_trips_lines.select(\n    safe_int_conversion_udf(\"year\").alias(\"year\"),\n    safe_int_conversion_udf(\"month\").alias(\"month\"),\n    safe_int_conversion_udf(\"day\").alias(\"day\"),\n    \"origin\",\n    \"destination\"\n)"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------+----------+\n|   destination|dest_count|\n+--------------+----------+\n|     SAO PAULO|     17102|\n|RIO DE JANEIRO|     14299|\n|BELO HORIZONTE|      4626|\n|        RECIFE|      2874|\n| FLORIANOPOLIS|      2857|\n+--------------+----------+\nonly showing top 5 rows\n\n"}], "source": "# Agrupamos los datos por 'destination' y contamos el n\u00famero de trayectos a cada destino.\n# Esto resulta en un nuevo DataFrame con la cuenta de trayectos por destino.\nbus_trips_destinationDF = bus_trips_lines.groupBy(\"destination\").count()\n\n# Renombramos la columna 'count' a 'dest_count' para claridad.\n# Esto refleja que el n\u00famero representa la cuenta de trayectos a cada destino.\nbus_trips_destinationDF = bus_trips_destinationDF.withColumnRenamed(\"count\", \"dest_count\")\n\n# Finalmente, ordenamos y mostramos los 5 destinos m\u00e1s populares basados en el n\u00famero de viajes.\n# Utilizamos la funci\u00f3n desc de pyspark.sql.functions para ordenar de manera descendente.\nbus_trips_destinationDF.orderBy(F.desc(\"dest_count\")).show(5)"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 2}